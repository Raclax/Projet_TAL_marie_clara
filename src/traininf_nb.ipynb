{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouvrir les CSV et diviser train en train et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>titre</th>\n",
       "      <th>type</th>\n",
       "      <th>difficulte</th>\n",
       "      <th>cout</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>recette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recette_221358.xml</td>\n",
       "      <td>Feuilleté de saumon et de poireau, sauce aux c...</td>\n",
       "      <td>Plat principal</td>\n",
       "      <td>Facile</td>\n",
       "      <td>Moyen</td>\n",
       "      <td>- 1 gros pavé de saumon - 100 g de crevettes d...</td>\n",
       "      <td>Couper finement le blanc et un peu de vert des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recette_48656.xml</td>\n",
       "      <td>Cake poulet/moutarde/amandes</td>\n",
       "      <td>Entrée</td>\n",
       "      <td>Très facile</td>\n",
       "      <td>Bon marché</td>\n",
       "      <td>- 3 œufs - 150 g de farine - 1 sachet de levur...</td>\n",
       "      <td>Couper finement l'échalote, la faire revenir à...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recette_30049.xml</td>\n",
       "      <td>Bûche à la truite fumée (7ème rencontre)</td>\n",
       "      <td>Entrée</td>\n",
       "      <td>Moyennement difficile</td>\n",
       "      <td>Assez Cher</td>\n",
       "      <td>- 800 g de filet de truite saumonnée fumée en ...</td>\n",
       "      <td>Faites blanchir les épinards à l'eau bouillant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recette_71424.xml</td>\n",
       "      <td>Gâteau au yaourt au coco sans huile de laetitia</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>Très facile</td>\n",
       "      <td>Bon marché</td>\n",
       "      <td>- 1 pot de yaourt - 1 pot de lait de coco - 3 ...</td>\n",
       "      <td>Mélanger dans l'ordre tous les ingrédients en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recette_217204.xml</td>\n",
       "      <td>Crêpes au canard laqué</td>\n",
       "      <td>Entrée</td>\n",
       "      <td>Moyennement difficile</td>\n",
       "      <td>Moyen</td>\n",
       "      <td>- 90 g de farine - 45 g de maïzena - 2 œufs - ...</td>\n",
       "      <td>Fouetter les œufs avec l'eau, le lait et le su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12468</th>\n",
       "      <td>recette_88545.xml</td>\n",
       "      <td>Tarte thon et courgettes</td>\n",
       "      <td>Plat principal</td>\n",
       "      <td>Facile</td>\n",
       "      <td>Bon marché</td>\n",
       "      <td>- 1 pâte feuilleté - 2 œufs - 2 courgettes moy...</td>\n",
       "      <td>Couper les courgettes en fines rondelles, les ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12469</th>\n",
       "      <td>recette_36440.xml</td>\n",
       "      <td>Gratin de légumes à la viande</td>\n",
       "      <td>Plat principal</td>\n",
       "      <td>Facile</td>\n",
       "      <td>Bon marché</td>\n",
       "      <td>- 500 g de pommes de terre  - 400 g de carotte...</td>\n",
       "      <td>Epluchez les pommes de terre et lavez-les. Cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>recette_174633.xml</td>\n",
       "      <td>Tagliatelles fraiches au Montepulciano</td>\n",
       "      <td>Plat principal</td>\n",
       "      <td>Difficile</td>\n",
       "      <td>Moyen</td>\n",
       "      <td>- Pour les tagliatelles : - 150 g de semoule f...</td>\n",
       "      <td>Je conseille de préparer les tagliatelles la v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>recette_43030.xml</td>\n",
       "      <td>Bavarois praliné pistache, coeur de mousse au ...</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>Difficile</td>\n",
       "      <td>Assez Cher</td>\n",
       "      <td>- Pour le biscuit Dacquoise : - 160 g de blanc...</td>\n",
       "      <td>Petits commentaires :J'ai indiqué les recettes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>recette_56276.xml</td>\n",
       "      <td>Grand tralala au chocolat</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>Difficile</td>\n",
       "      <td>Moyen</td>\n",
       "      <td>- Pour la base craquante: - 60 g de crêpes den...</td>\n",
       "      <td>Faites fondre les 150 g de chocolat pour la fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12473 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc_id                                              titre  \\\n",
       "0      recette_221358.xml  Feuilleté de saumon et de poireau, sauce aux c...   \n",
       "1       recette_48656.xml                       Cake poulet/moutarde/amandes   \n",
       "2       recette_30049.xml           Bûche à la truite fumée (7ème rencontre)   \n",
       "3       recette_71424.xml    Gâteau au yaourt au coco sans huile de laetitia   \n",
       "4      recette_217204.xml                             Crêpes au canard laqué   \n",
       "...                   ...                                                ...   \n",
       "12468   recette_88545.xml                           Tarte thon et courgettes   \n",
       "12469   recette_36440.xml                      Gratin de légumes à la viande   \n",
       "12470  recette_174633.xml             Tagliatelles fraiches au Montepulciano   \n",
       "12471   recette_43030.xml  Bavarois praliné pistache, coeur de mousse au ...   \n",
       "12472   recette_56276.xml                          Grand tralala au chocolat   \n",
       "\n",
       "                 type             difficulte        cout  \\\n",
       "0      Plat principal                 Facile       Moyen   \n",
       "1              Entrée            Très facile  Bon marché   \n",
       "2              Entrée  Moyennement difficile  Assez Cher   \n",
       "3             Dessert            Très facile  Bon marché   \n",
       "4              Entrée  Moyennement difficile       Moyen   \n",
       "...               ...                    ...         ...   \n",
       "12468  Plat principal                 Facile  Bon marché   \n",
       "12469  Plat principal                 Facile  Bon marché   \n",
       "12470  Plat principal              Difficile       Moyen   \n",
       "12471         Dessert              Difficile  Assez Cher   \n",
       "12472         Dessert              Difficile       Moyen   \n",
       "\n",
       "                                             ingredients  \\\n",
       "0      - 1 gros pavé de saumon - 100 g de crevettes d...   \n",
       "1      - 3 œufs - 150 g de farine - 1 sachet de levur...   \n",
       "2      - 800 g de filet de truite saumonnée fumée en ...   \n",
       "3      - 1 pot de yaourt - 1 pot de lait de coco - 3 ...   \n",
       "4      - 90 g de farine - 45 g de maïzena - 2 œufs - ...   \n",
       "...                                                  ...   \n",
       "12468  - 1 pâte feuilleté - 2 œufs - 2 courgettes moy...   \n",
       "12469  - 500 g de pommes de terre  - 400 g de carotte...   \n",
       "12470  - Pour les tagliatelles : - 150 g de semoule f...   \n",
       "12471  - Pour le biscuit Dacquoise : - 160 g de blanc...   \n",
       "12472  - Pour la base craquante: - 60 g de crêpes den...   \n",
       "\n",
       "                                                 recette  \n",
       "0      Couper finement le blanc et un peu de vert des...  \n",
       "1      Couper finement l'échalote, la faire revenir à...  \n",
       "2      Faites blanchir les épinards à l'eau bouillant...  \n",
       "3      Mélanger dans l'ordre tous les ingrédients en ...  \n",
       "4      Fouetter les œufs avec l'eau, le lait et le su...  \n",
       "...                                                  ...  \n",
       "12468  Couper les courgettes en fines rondelles, les ...  \n",
       "12469  Epluchez les pommes de terre et lavez-les. Cou...  \n",
       "12470  Je conseille de préparer les tagliatelles la v...  \n",
       "12471  Petits commentaires :J'ai indiqué les recettes...  \n",
       "12472  Faites fondre les 150 g de chocolat pour la fe...  \n",
       "\n",
       "[12473 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../data/train1.csv\", index=False)\n",
    "df_validation.to_csv(\"../data/validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'fr_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,lemma\")\u001b[39;00m\n\u001b[32m      2\u001b[39m     \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# def lemmatize(text):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# df_train.to_csv(\"../data/train_pre.csv\", index=False)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr_core_news_lg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_text\u001b[39m(text):\n\u001b[32m     21\u001b[39m     doc = nlp(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     28\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     29\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     35\u001b[39m ) -> Language:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/spacy/util.py:472\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'fr_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,lemma\")\n",
    "    \n",
    "# def lemmatize(text):\n",
    "#     doc = nlp(text)\n",
    "#     lemmas = [\n",
    "#         word.lemma\n",
    "#         for sentence in doc.sentences\n",
    "#         for word in sentence.words\n",
    "#     ]\n",
    "#     return \" \".join(lemmas)\n",
    "\n",
    "# # faire une boucle pour ne prendre qu'une recette à la fois\n",
    "# for i in range(len(df_train)):\n",
    "#     df_train[\"recette\"][i] = df_train[\"recette\"][i].lower()\n",
    "#     df_train[\"recette\"][i] = lemmatize(df_train[\"recette\"][i])\n",
    "    \n",
    "# df_train.to_csv(\"../data/train_pre.csv\", index=False)\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "\n",
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df_train['cleaned_text'] = df_train['text'].apply(clean_text)\n",
    "df_train['cleaned_text'] = df_train['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text\"] = df_train[\"titre\"] + \" \" + df_train[\"recette\"]\n",
    "df_test[\"text\"] = df_test[\"titre\"] + \" \" + df_test[\"recette\"]\n",
    "df_validation[\"text\"] = df_validation[\"titre\"] + \" \" + df_validation[\"recette\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline (random?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m y_validation = df_validation[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# compter la taille de chaque classe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mlen\u001b[39m( y_train))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/scipy/sparse/_base.py:425\u001b[39m, in \u001b[36m_spbase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    426\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33m or shape[0]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(df_test['text'])\n",
    "x_test = vectorizer.transform(df_test['text'])\n",
    "x_validation = vectorizer.transform(df_validation['text'])\n",
    "\n",
    "y_train = df_train['type']\n",
    "y_test = df_test['type']\n",
    "y_validation = df_validation['type']\n",
    "\n",
    "# compter la taille de chaque classe\n",
    "print(len(x_train), len( y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1388, 9978]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     y_pred = cross_val_predict(dt_classifier, matrix, classes, cv=\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mdecision_tree\u001b[39m\u001b[34m(matrix, classes)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecision_tree\u001b[39m(matrix, classes):\n\u001b[32m      2\u001b[39m     dt_classifier = DecisionTreeClassifier()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     y_pred = \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1174\u001b[39m, in \u001b[36mcross_val_predict\u001b[39m\u001b[34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate cross-validated estimates for each input data point.\u001b[39;00m\n\u001b[32m   1045\u001b[39m \n\u001b[32m   1046\u001b[39m \u001b[33;03mThe data is split according to the cv parameter. Each sample belongs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1171\u001b[39m \u001b[33;03m>>> y_pred = cross_val_predict(lasso, X, y, cv=3)\u001b[39;00m\n\u001b[32m   1172\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1173\u001b[39m _check_groups_routing_disabled(groups)\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m X, y = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1175\u001b[39m params = {} \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m params\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[32m   1178\u001b[39m     \u001b[38;5;66;03m# For estimators, a MetadataRouter is created in get_metadata_routing\u001b[39;00m\n\u001b[32m   1179\u001b[39m     \u001b[38;5;66;03m# methods. For these router methods, we create the router to use\u001b[39;00m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;66;03m# `process_routing` on it.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/utils/validation.py:532\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    503\u001b[39m \n\u001b[32m    504\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    531\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/utils/validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [1388, 9978]"
     ]
    }
   ],
   "source": [
    "def decision_tree(matrix, classes):\n",
    "    dt_classifier = DecisionTreeClassifier()\n",
    "    y_pred = cross_val_predict(dt_classifier, matrix, classes, cv=10)\n",
    "    return y_pred\n",
    "\n",
    "decision_tree(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1388, 9978]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m y_train = encoder.fit_transform(y_train)\n\u001b[32m     10\u001b[39m model = SVC(kernel=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# model_cleaned = SVC(kernel='linear')\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# y_pred = cross_val_predict(model, x_train, y_train, cv=10)\u001b[39;00m\n\u001b[32m     16\u001b[39m y_test = encoder.transform(df_test[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/svm/_base.py:197\u001b[39m, in \u001b[36mBaseLibSVM.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    195\u001b[39m     check_consistent_length(X, y)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m y = \u001b[38;5;28mself\u001b[39m._validate_targets(y)\n\u001b[32m    209\u001b[39m sample_weight = np.asarray(\n\u001b[32m    210\u001b[39m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype=np.float64\n\u001b[32m    211\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/utils/validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/utils/validation.py:1389\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1370\u001b[39m X = check_array(\n\u001b[32m   1371\u001b[39m     X,\n\u001b[32m   1372\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1384\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1385\u001b[39m )\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M12/S2/TAL/praat/lib/python3.11/site-packages/sklearn/utils/validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [1388, 9978]"
     ]
    }
   ],
   "source": [
    "def svm(matrix, classes):\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "    y_pred = cross_val_predict(svm_classifier, matrix, classes, cv=10)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# model_cleaned = SVC(kernel='linear')\n",
    "# y_pred = cross_val_predict(model, x_train, y_train, cv=10)\n",
    "\n",
    "y_test = encoder.transform(df_test['label'])\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "#model_cleaned.fit(x_train_cleaned, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(matrix, classes):\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    y_pred = cross_val_predict(rf_classifier, matrix, classes, cv=10)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(matrix, classes):\n",
    "    nb_classifier = MultinomialNB()\n",
    "    y_pred = cross_val_predict(nb_classifier, matrix, classes, cv=10)\n",
    "    return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
